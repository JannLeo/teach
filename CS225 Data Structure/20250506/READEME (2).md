![image-20250509192310434](./READEME (2).assets/image-20250509192310434.png)

这张图解释了 **Bloom Filter 的错误率（False Positive Rate, FPR）**，重点是：在插入了 `n` 个元素之后，我们期望布隆过滤器的某一位被错误地判断为 “已存在” 的概率。

------

### 图中内容逐项解释如下：

#### 标题

- **Bloom Filter: Error Rate**：这是关于布隆过滤器误判率的公式推导。

------

### 公式部分：

我们关注的是：

> **某一位在插入 `n` 个对象之后，仍然是 1 的概率**

这代表了我们可能发生**假阳性（False Positive）**的概率，也就是你查询一个没插入过的元素，它却被判定“存在”的可能性。

------

#### 各变量含义：

- `m`：布隆过滤器的比特位总数（bit vector 的长度）
- `k`：哈希函数的数量
- `n`：插入的元素个数

------

### 正文黑体问题：

> **What is our expected FPR after n objects are inserted?**

我们的问题是：**插入 `n` 个对象后，误判率（FPR）是多少？**

------

### 蓝色文字：概率解释

> **The probability my bit is 1 after n objects inserted**

这个概率是：

$\left(1 - \left(1 - \frac{1}{m} \right)^{nk} \right)^k$

逐层解释：

1. **内层部分**：
   - $\left(1 - \frac{1}{m} \right)^{nk}$：表示某一位 **没有被任何一个哈希函数设置为 1** 的概率。
     - 每个插入元素用 `k` 个哈希函数，每次命中概率是 $\frac{1}{m}$
     - 总共 `nk` 次尝试
2. **外层部分**：
   - $1 - \text{上式}$：就是至少一次命中的概率（即变成1的概率）
3. **整体加上幂 `k`**：
   - 表示用 `k` 个哈希函数查询一个没插入过的元素时，**每个哈希函数对应的位都为 1** 的概率，也就是假阳性率。

------

### 右侧图示：

- 显示的是一个 Bloom Filter 比特向量，长度为 `m`
- 某个位置被标记为 1，代表它可能被若干哈希函数命中
- 使用的是 `k` 个哈希函数（底部标注 `h_{1,2,...,k}`）

------

### 橙色小字：

> **The number of [assumed independent] trials**

说明这个概率公式建立在：**每一次 hash 操作是独立的** 假设下。

------

### 总结：

这个公式是计算布隆过滤器误判率的核心，展示了它如何受到 `m`（位数组长度）、`k`（哈希函数数）、`n`（插入数）的影响。

是否需要我用这个公式举一个实际计算例子？

![image-20250509192600434](./READEME (2).assets/image-20250509192600434.png)

这张图是对前一页的延续，进一步探讨如何**最小化 Bloom Filter 的误判率（False Positive Rate, FPR）**，提出一个关键问题：

------

## **核心问题：**

> **To minimize the FPR, do we prefer…**
>  **(A) large k 还是 (B) small k？**

也就是说：**选择更多还是更少的哈希函数**，哪个更能降低误判率？

------

### 图中公式回顾：

$\left(1 - \left(1 - \frac{1}{m} \right)^{nk} \right)^k$

这是 FPR 的表达式。变量含义如下：

- `m`: Bloom Filter 的位数组长度
- `k`: 哈希函数数量
- `n`: 插入元素数量

------

## **k 的影响：**

我们用直觉来分析：

### 1. **当 k 较小时：**

- 每个元素只设置少数位，bit vector 中被置 1 的位较少。
- 查询时命中全部位的可能性较低，**误判率低**。

但也有可能：

- 太少的哈希函数 → 利用率不高 → 精度差。

------

### 2. **当 k 较大时：**

- 每个元素设置更多位 → bit vector 更容易“饱和”。
- 所有查询的哈希结果都命中为 1 的概率升高 → **误判率升高**！

------

## **那最佳值是多少？**

实际中有一个**最优的 k 值**，使误判率最低：

$k^* = \frac{m}{n} \ln 2$

也就是：给定 `m` 和 `n`，我们可以计算出最理想的哈希函数数量 `k`。这是下一页通常会介绍的内容。

------

### 图右侧补充：

- 是一个长度为 `m` 的 bit vector。
- 使用了 `k` 个哈希函数 `h₁, h₂, ..., hₖ`，每个用来设置一个 bit。

------

### 小结：

- 图中是引导学生思考：**哈希函数数量太多或太少都不是好事**。
- 我们要根据公式分析 FPR 和 `k` 的关系，最终找到 **最小化 FPR 的最优 k 值**。

------

是否需要我带你推导出最优 `k` 值的公式，或画个图展示它和误判率的关系？

![image-20250509192702587](./READEME (2).assets/image-20250509192702587.png)

这张图进一步解释了上两页提出的问题：**“选大的 k 还是小的 k 更能降低 Bloom Filter 的误判率？”**

------

## **左侧：A. 选择更大的 k**

### 数学含义：

表达式：

$\left(1 - \left(1 - \frac{1}{m} \right)^{nk} \right)^k$

- `k` 越大，`nk` 就越大 → 会使得整个 Bloom Filter 的 bit 被**快速填满**

- 所以整个表达式趋近于：

  (1−e−nk/m)k→1\left(1 - e^{-nk/m} \right)^k \to 1

- 导致 FPR（误判率）**趋近于 100%**

### 图中解释（红色）：

- 写着 “**FPR approaches 1**” 和 “≡ 100% FPR”
- 画了一个布隆过滤器示意图，全都是 `1`，表示**“bit vector 全部被占用了”**
- 这意味着：任何查询都会命中 `k` 个 1 → 会错误地认为元素存在

------

## **右侧：B. 选择较小的 k**

### 数学含义：

- 如果 `k` 太小，虽然 bit vector 比较稀疏，**不会太容易误判**
- 但缺点是：
  - hash 覆盖不够广 → 容易误判一些重复项
  - 整体效果仍然不好

图中蓝字解释：

- “As k decreases, this gets smaller!” 强调的是“FPR 可能变小”，但同时也要注意这不是无限下降的
- 蓝色图示中写了：
  - “Repeated random trials”：指的是我们可以看作每个哈希查询是一次独立试验
  - $FPR = (P)^k$：说明误判率是多个概率乘积，k 越小，次数越少 → 总概率降低

------

## **总结：**

- **k 太大 → bit vector 很快填满 → 误判率接近 100%**

- **k 太小 → 哈希函数少 → 每个查询信息不足，也可能导致误判率上升**

- **最优 k 值在中间**，即：

  k∗=ln⁡2⋅mnk^* = \ln 2 \cdot \frac{m}{n}

  这个值最小化 FPR，后续 PPT 通常会介绍这个推导。

------

是否要我帮你画图或解释一下这个最优 k 值的推导过程？

![image-20250509193035008](./READEME (2).assets/image-20250509193035008.png)

这张图是对 **Bloom Filter 最佳误判率（FPR）** 的推导，目标是找出 **最优哈希函数数量 `k\*`**。我们来详细解读每一部分：

------

## 一、目标：

> **To build the optimal hash function, fix `m` and `n`!**

我们想在 `m`（bit 向量长度）和 `n`（插入元素数量）固定的情况下，找到使误判率最小的 `k` 值。

------

## 二、关键结论：

$\boxed{k^* = \ln 2 \cdot \frac{m}{n}}$

这就是 **最优哈希函数数量的公式**。

- `ln 2 ≈ 0.693`，是自然对数常数
- 它告诉我们：**理想的哈希数量 k 与位图长度和元素个数有关**
- 右边蓝笔注释：
  - `k = hash` 函数个数
  - `M = BF size` 是布隆过滤器的位数
  - `n = items` 插入项数量
  - `α = n/m` 是负载因子（load factor）

------

## 三、公式推导过程：

### 原始 FPR 公式：

$\left(1 - \left(1 - \frac{1}{m}\right)^{nk}\right)^k$

这是布隆过滤器发生误判的概率。它很复杂，于是我们使用近似展开来简化它。

------

### 步骤 (1)：用自然指数函数逼近

使用 **泰勒展开** 得到：

$\left(1 - \left(1 - \frac{1}{m}\right)^{nk}\right)^k \approx \left(1 - e^{-nk/m}\right)^k$

右边红色注释指出，这个形式更容易微分、分析形状。

------

### 步骤 (2)：对近似表达式求导，寻找最小值

我们想让 FPR 最小，所以对误判率关于 `k` 的导数求导：

$\frac{d}{dk} \left(1 - e^{-nk/m}\right)^k \approx \frac{d}{dk} \left[k \cdot \ln(1 - e^{-nk/m})\right]$

这一技巧是常见的优化策略：**最小化 f(k) 就等价于最小化 ln(f(k))**，因为对数函数是单调的。

最终我们可以解出最小值出现在：

$k^* = \ln 2 \cdot \frac{m}{n}$

------

## 四、总结

这页 PPT 给出了**最优布隆过滤器设计的关键定理**：

- **最优哈希函数数量 k\*** 不固定，而是由 `m` 和 `n` 决定的
- 如果你知道要插入 `n` 个元素，准备了 `m` 个 bit，你就可以计算理想的哈希函数数量

------

是否需要我帮你做一个实际计算或画图演示 k 和 FPR 的关系？

![image-20250509193227740](./READEME (2).assets/image-20250509193227740.png)

这张图是对 Bloom Filter 最佳误判率推导中的 **Claim 1 的详细证明**，使用了指数函数和对数的性质来简化复杂表达式。我们逐行解释：

------

## **Claim 1 内容：**

$\left(1 - \left(1 - \frac{1}{m} \right)^{nk}\right)^k \approx \left(1 - e^{-nk/m} \right)^k$

也就是说：
 **布隆过滤器的误判率表达式可以近似为**：

从左边复杂的形式 → 右边使用指数函数的形式
 这样便于微分、分析和寻找最优值。

------

## **推导重点：底层用到的数学恒等式**

下面这行就是对中间那一块：

$\left(1 - \frac{1}{m} \right)^{nk}$

的处理方式：

$\left(1 - \frac{1}{m} \right)^{nk} = e^{\ln\left(\left(1 - \frac{1}{m} \right)^{nk}\right)} = e^{nk \cdot \ln(1 - 1/m)}$

这是指数与对数互换的基本性质：

- ![image-20250509193325762](./READEME (2).assets/image-20250509193325762.png)

这一步就是将幂转化为指数函数，用于数学推导简化。

------

## **为什么这样变形？**

- 为了简化表达式，将乘方变为指数，有利于微分。
- 因为：
  - 指数函数 `e^x` 的导数仍是 `e^x`，便于求最小值。
  - `ln(1 - 1/m)` 也能用泰勒展开进一步逼近。

------

## **下一步将近似：**

使用泰勒展开：

$\ln(1 - x) \approx -x \quad \text{当 } x \ll 1$

所以：

$\left(1 - \frac{1}{m} \right)^{nk} \approx e^{-nk/m}$

------

## **最终目标：**

将整个 FPR 公式：

$\left(1 - \left(1 - \frac{1}{m} \right)^{nk} \right)^k$

简化为：

$\left(1 - e^{-nk/m} \right)^k$

为下一步的优化和导数分析打下基础。

------

如你希望，我可以继续解释后续如何求导并得到最优哈希数 `k* = ln2·m/n`。是否继续？

![image-20250509193408225](./READEME (2).assets/image-20250509193408225.png)

这张图展示了布隆过滤器推导中的核心近似过程：**利用泰勒展开将幂函数近似为指数函数**。这是整个 FPR 公式简化的关键一步。我们来详细解释：

------

## 一、标题：

**Bloom Filter: Optimal Error Rate**
 这一页要说明为什么下面这个近似成立：

$\left(1 - \frac{1}{m} \right)^{nk} \approx e^{-nk/m}$

------

## 二、关键知识：泰勒展开（Mercator Series）

图中上方写的是自然对数的泰勒展开：

$\ln(1 + x) = x - \frac{x^2}{2} + \frac{x^3}{3} - \frac{x^4}{4} + \cdots$

这是经典的 Mercator 级数展开，适用于 $|x| < 1$。

图中红色把二次项以后的部分都划掉了，表示我们**只保留第一项**来进行近似。

------

## 三、实际应用在布隆过滤器上

我们正在处理的表达式是：

$\left(1 - \frac{1}{m} \right)^{nk}$

取对数：

$\ln \left(1 - \frac{1}{m} \right) \approx -\frac{1}{m}$

因此：

$\left(1 - \frac{1}{m} \right)^{nk} = e^{nk \cdot \ln(1 - \frac{1}{m})} \approx e^{-nk/m}$

图中写了：

- $x = -\frac{1}{m}$，是小量

- 所以可以认为：

  ![image-20250509193723135](./READEME (2).assets/image-20250509193723135.png)

- 整个变换就是为了简化这个表达式，让它更容易求导和优化。

------

## 四、最终结论（图右下角黑框中的结论）

$\boxed{ \left(1 - \frac{1}{m} \right)^{nk} \approx e^{-nk/m} }$

这个近似成立的前提是：**m 很大、1/m 很小**，即 Bloom Filter 比特数组较长时是合理的。

------

## 总结一下：

- 本页完成了将复杂幂函数简化为指数函数的关键一步

- 接下来我们就能用这条近似结果，把布隆过滤器误判率公式写成：

  ![image-20250509193734486](./READEME (2).assets/image-20250509193734486.png)

  然后对它求导，得出最优哈希函数个数。

------

需要我继续解释如何根据这个近似值求导并找出最优 `k*` 吗？

![image-20250509193757025](./READEME (2).assets/image-20250509193757025.png)

这张图是对前面几页近似推导的最终收束，展示如何通过求导来找到 **使 Bloom Filter 的误判率最小的最优哈希函数数量 $k^\*$**。我们来逐步解释：

------

## 一、标题：Optimal Error Rate

> **目标**：对 Bloom Filter 的 FPR 近似表达式关于哈希函数数量 $k$ 求导，找出最优值。

------

## 二、Claim 2：求导步骤

我们已将误判率近似为：

$f(k) = \left(1 - e^{-nk/m} \right)^k$

这个形式不好直接求导，于是我们取对数，变成：

$\ln f(k) = k \cdot \ln(1 - e^{-nk/m})$

于是可以用链式法则求导：

$\frac{d}{dk} f(k) \approx \frac{d}{dk} \left[k \cdot \ln(1 - e^{-nk/m})\right]$

这是图中右上红笔圈出的公式。

------

## 三、数学技巧：导数规则

图中中间的公式是关键：

$\frac{d}{dx} \ln f(x) = \frac{1}{f(x)} \cdot \frac{df(x)}{dx}$

这是**对数函数的导数链式法则**，它让我们能先求 $\ln f(k)$ 的导数，然后再回到 $f(k)$ 上。

图中红笔注释：“Take deriv. of ln”，就是这个意思。

------

## 四、最小值结论（TL;DR）

图中说：

> \min f(x) = \min \ln f(x) ]

这在数学上是成立的（因为 $\ln(x)$ 是单调递增的），所以我们只要找 $\ln f(k)$ 的最小点，就等于找 $f(k)$ 的最小点。

------

## 五、最终结论：

> **导数为 0 时**，达到最小误判率的点：

$\boxed{k^* = \ln 2 \cdot \frac{m}{n}}$

这就完成了整个最优哈希数的推导过程！

------

### 小结：

- 使用指数近似和泰勒展开简化表达式
- 用链式法则求导
- 找到使导数为 0 的 $k$，即为最优解

------

如果你需要我进一步代入数字算一个实际例子（比如 m=1000, n=100），或者画图可视化 FPR 随 k 的变化，我也可以继续补充。是否需要？

非常好，我们来**详细推导出 Bloom Filter 的误判率最小时最优的哈希函数数量 $k^\*$**。这个过程包含如下几个步骤：

------

## **目标：**

最小化 FPR 近似表达式：

$f(k) = \left(1 - e^{-nk/m} \right)^k$

------

## **步骤 1：对数简化（取 ln）**

我们对 $f(k)$ 取自然对数（方便求导）：

$\ln f(k) = k \cdot \ln\left(1 - e^{-nk/m} \right)$

------

## **步骤 2：对上式求导**

记住：我们现在是要求导数 $\frac{d}{dk} \ln f(k)$，再让它等于 0 来找最小值。

使用积的求导法则：

$\frac{d}{dk} \left[ k \cdot \ln\left(1 - e^{-nk/m} \right) \right] = \ln\left(1 - e^{-nk/m} \right) + k \cdot \frac{d}{dk} \ln\left(1 - e^{-nk/m} \right)$

现在我们来算第二项：

------

## **步骤 3：链式法则继续求导**

我们有：

$\frac{d}{dk} \ln\left(1 - e^{-nk/m} \right) = \frac{1}{1 - e^{-nk/m}} \cdot \frac{d}{dk} \left(1 - e^{-nk/m} \right)$$= \frac{1}{1 - e^{-nk/m}} \cdot \left( 0 - \left( \frac{n}{m} \cdot e^{-nk/m} \right) \right) = - \frac{n}{m} \cdot \frac{e^{-nk/m}}{1 - e^{-nk/m}}$

------

## **步骤 4：代入回总导数表达式**

$\frac{d}{dk} \ln f(k) = \ln\left(1 - e^{-nk/m} \right) - \frac{nk}{m} \cdot \frac{e^{-nk/m}}{1 - e^{-nk/m}}$

令导数为 0：

$\ln\left(1 - e^{-nk/m} \right) = \frac{nk}{m} \cdot \frac{e^{-nk/m}}{1 - e^{-nk/m}}$

这是一个复杂的方程，不容易代数上解出。但我们可以通过数值方式或图像观察最小点在哪。

------

## **步骤 5：用近似和数学分析得到结论**

使用数学推导（例如使用极值点、导数二阶分析等方法）可以得出：

$k^* = \ln 2 \cdot \frac{m}{n}$

这是已知的数学最优解，在实际中被广泛采用。

------

## **结论：**

只要你知道：

- $m$：布隆过滤器 bit vector 的总长度
- $n$：要插入的元素个数

你就可以算出**最优哈希函数数量**：

$\boxed{k^* = \ln 2 \cdot \frac{m}{n} \approx 0.693 \cdot \frac{m}{n}}$

------

## **例子：**

假设：

- $m = 1000$（1000 位 bit 向量）
- $n = 100$（插入 100 个元素）

则：

$k^* = \ln 2 \cdot \frac{1000}{100} = 0.693 \cdot 10 \approx 6.93 \approx 7$

**最优选择是 7 个哈希函数。**

------

如果你想，我可以画图展示误判率随 $k$ 变化的曲线，帮助你更直观地理解这一结论。需要吗？

![image-20250509194214147](./READEME (2).assets/image-20250509194214147.png)这张图以图像方式直观展示了我们刚才推导的布隆过滤器最优误判率（FPR）点是如何得到的。下面是详细解释：

------

## **图表左侧公式：**

误判率 FPR 的近似公式为：

$\text{FPR} \approx \left(1 - e^{-nk/m}\right)^k$

这是我们之前几页推导出的核心公式。

------

## **图表说明：**

- 横轴（x轴）：**哈希函数数量 $k$**，从 4 到 9 个
- 纵轴（y轴）：**误判率 FPR**
- 图中展示了不同 $k$ 值下 FPR 的变化趋势，形成一个 **U 型曲线**

------

## **红色分析：**

- **左侧**：$k$ 太小 → FPR 较高
  - 注解写着：**“Not enough random trials”**
  - 原因：哈希函数太少，每个元素只设置少数几位，容易“撞车”
  - 布隆过滤器的能力未被充分利用
- **右侧**：$k$ 太大 → FPR 也升高
  - 注解写着：**“BF becomes too saturated with 1s”**
  - 原因：太多哈希函数使得 bit 向量被大量填充为 1
  - 结果是几乎所有位置都为 1，查询任意值几乎都误判“存在”

------

## **最优点标注：**

- 红色箭头指出最小点在：

  k∗=ln⁡2⋅mn=ln⁡2⋅10≈6.93k^* = \ln 2 \cdot \frac{m}{n} = \ln 2 \cdot 10 \approx 6.93

- 图中曲线最低点正好在 $k \approx 7$，验证了理论推导的结果

------

## **右上角说明 `m/n = 10`**

- 表示：
  - Bit vector 长度是元素数量的 10 倍
  - 是布隆过滤器设计中的一个“负载因子”
- 这个比例常用于实际部署

------

## **右侧图示：**

- 一个小框图表示 bit 向量中大量位置被设置成 1（饱和），意味着误判率趋近于 1

------

## **总结：**

- 图像 **完美验证** 数学推导结果

- **FPR 随着哈希函数数量 $k$** 先下降后上升 → 存在最优点

- 我们通过数学推导得出：

  k∗=ln⁡2⋅mnk^* = \ln 2 \cdot \frac{m}{n}

  并通过图像进一步直观确认了这一点

------

如你需要，我可以帮你制作一个交互图表（用你自己的参数）或用 Python 画出不同 m/n 比下的 FPR 曲线图。需要吗？

![image-20250509195304020](./READEME (2).assets/image-20250509195304020.png)

这张图总结了 Bloom Filter 中三个关键参数的关系，并展示了如何通过已知两个参数来计算第三个。我们来详细分析：

------

## 一、核心公式：

$\boxed{k^* = \ln 2 \cdot \frac{m}{n}}$

- $k^*$：最优哈希函数数量（为了最小化误判率）
- $m$：布隆过滤器的 bit 向量长度（单位：bit）
- $n$：插入元素个数

> **只要知道其中两个参数，就可以计算出第三个。**

------

## 二、逐个案例解释

------

### **第一行：求 $m$**

- 已知：$n = 100$，$k = 3$
- 用公式反推：

$m = \frac{k \cdot n}{\ln 2} = \frac{3 \cdot 100}{0.693} \approx 433 \text{ bits}$

也就是说，为了在插入 100 个元素、使用 3 个哈希函数时达到最优，bit 向量需要至少 433 位。

------

### **第二行：求 $k$**

- 已知：$m = 100$，$n = 20$
- 代入公式：

$k = \ln 2 \cdot \frac{m}{n} = 0.693 \cdot \frac{100}{20} = 3.465 \approx 3.5$

可以选择四个哈希函数，更接近最优点。

------

### **第三行：求 $n$**

- 已知：$m = 100$，$k = 2$
- 反推：

$n = \frac{m \cdot \ln 2}{k} = \frac{100 \cdot 0.693}{2} \approx 34.65 \approx 35 \text{ items}$

所以当我们希望只用 2 个哈希函数，并希望保持最优误判率，插入元素应控制在 35 个以内。

------

## 三、总结记忆方式：

| 已知       | 求解公式                      |
| ---------- | ----------------------------- |
| $n$ 和 $k$ | $m = \frac{k \cdot n}{\ln 2}$ |
| $m$ 和 $n$ | $k = \ln 2 \cdot \frac{m}{n}$ |
| $m$ 和 $k$ | $n = \frac{m \cdot \ln 2}{k}$ |

------

这张图展示的非常实用：**如何在资源受限的系统设计中做权衡取值**。

是否需要我提供一个 Excel 表格或 Python 脚本，方便你自由输入两个参数快速计算第三个？

![image-20250509195355287](./READEME (2).assets/image-20250509195355287.png)

这张图是 Bloom Filter 最佳参数选择在真实大规模应用中的一个**实践案例分析**，以 “1000 Genomes Project” 为例进行估算，展示了 Bloom Filter 的惊人节省空间能力。我们分解解释如下：

------

## 一、上方公式（重点）

$\boxed{m = \frac{n \cdot k}{\ln 2} \approx 1.44 \cdot n \cdot k}$

含义是：为了实现最佳误判率，**bit 向量长度 `m` 应该约等于 1.44 倍的 `n·k`**

------

## 二、红笔说明解析

### **右上角标注**

- “Optimal hash function is still O(1)!”：即便要使用多个哈希函数，复杂度仍然是常数级，不影响性能。
- “Don’t need to be optimal!”：在实际系统中，**哪怕不是最优参数，只要在合理范围内，Bloom Filter 效果仍然很好。**

------

## 三、实际例子分析：

### **场景：1000 Genomes Project**

- `n = 250,000 files` （估计的基因样本或文件数）
- 数据规模 `≈ 10^{15}` nucleotides（核苷酸序列数量）
- 原始数据大小：**260 TB**

------

### **我们用 Bloom Filter 来索引这些文件：**

用公式估算 bit 向量所需大小：

- 设 $k = 1$ 或 $k = 2$

- 使用：

  m≈1.44⋅n⋅km \approx 1.44 \cdot n \cdot k

- 带入 $n = 250,000$：

#### 当 $k = 1$：

$m = 1.44 \cdot 250,000 \approx 360,000 \text{ bits} ≈ 45 \text{ KB}$

#### 当 $k = 2$：

$m ≈ 720,000 \text{ bits} ≈ 90 \text{ KB}$

所以：

- **只用 45–90 KB 就能代表 260 TB 的数据文件集合索引信息！**
- 布隆过滤器用于“快速判断某个文件是否可能存在”，非常高效。

------

## 四、底部说明：

- **搜索词“cat photos”** 引出了另一个场景：海量网页图像也可以用布隆过滤器表示。
- 提到了：
  - `n = 60 billion ~ 130 trillion`：Google 网页总量（Bloom Filter 可用于 URL 黑名单、安全检测等）

------

## 五、总结：

| 对比项   | 原始存储                         | Bloom Filter                          |
| -------- | -------------------------------- | ------------------------------------- |
| 大小     | 260 TB                           | 45–90 KB                              |
| 精度     | 有一定误判（可控）               | 有误判但**无漏判**（One-sided error） |
| 应用场景 | 文件存在性检测、搜索索引、黑名单 | 极其适合大规模系统索引和检测          |

------

如你感兴趣，我可以继续用 Python 模拟“指定误判率下如何选择 m 和 k”，或将它写成 Excel 模板方便调整，是否需要？

![image-20250509195516561](./READEME (2).assets/image-20250509195516561.png)

这张图是对 Bloom Filter 的**关键概念与误判率公式**的总结，内容精炼、概括性强。我们来逐条解释：

------

## **一、标题：Bloom Filters**

说明 Bloom Filter 是：

> **A probabilistic data structure storing a set of values**

它是一种**概率性数据结构**，用来表示一个集合，可以高效判断某元素是否“可能存在”或“一定不存在”。

------

## **二、三个关键参数：**

1. **$k$** — 哈希函数数量
   - 每个插入元素通过 $k$ 个哈希函数映射到 $k$ 个 bit 位
2. **$n$** — 插入的元素数量
   - 是你计划往 Bloom Filter 中加入的项目个数
3. **$m$** — bit 向量的总长度（单位：bit）
   - 决定了 Bloom Filter 的总内存空间

------

## **三、误判率公式：**

### 精确表达：

$\left(1 - \left(1 - \frac{1}{m} \right)^{nk} \right)^k$

解释：

- $\left(1 - \frac{1}{m} \right)^{nk}$：表示一个 bit 没被设置为 1 的概率
- 外面再取 $1 - (\cdot)$：表示某个 bit 被设置成 1 的概率
- 然后整个表达式的 $k$ 次方：表示查询一个元素时，**所有 k 个位置都为 1** 的概率，即 **误判率 FPR**

### 指数近似：

$\left(1 - e^{-nk/m} \right)^k$

这是前几页推导中使用泰勒展开后的近似形式，便于进一步分析。

------

## **四、最优准确率条件：**

$\boxed{k^* = \ln 2 \cdot \frac{m}{n}}$

含义：

- 当哈希函数数量取这个值时，误判率最低
- 是整个布隆过滤器设计中最重要的公式之一
- 已在多页图中被推导验证

------

## **五、右侧图示：**

- 一个纵向 bit 向量，总共 $m$ 位
- 右上标注 $h_{\{1,2,...,k\}}$：表示有 $k$ 个哈希函数来控制它
- 上方的计时器图标暗示 Bloom Filter 查询非常快（**常数时间 O(k)**）

------

## **总结：**

这页是对你整套 Bloom Filter 学习的高度概括：

| 概念       | 公式/含义                          |
| ---------- | ---------------------------------- |
| FPR        | $\left(1 - e^{-nk/m} \right)^k$    |
| 最优哈希数 | $k^* = \ln 2 \cdot \frac{m}{n}$    |
| 参数关系   | 只需设定两个参数，另一个可以反推出 |
| 结构性质   | 占用少，查询快，误判但无漏判       |

------

如果你需要我帮你把这些公式/关系整理成一个可打印的公式卡片或知识图谱，我也可以制作。是否需要？

**Cardinality Sketch（基数估计草图）** 是一种 **节省空间的算法工具**，用来在不存储所有数据的前提下，**估计一个集合中有多少个不同（unique）的元素**。它常用于处理**大规模数据流**或**数据集合**，比如：

- 互联网用户访问记录
- 搜索关键词去重
- DNA片段唯一性分析
- 网络数据包识别
- 日志系统中独立 IP 数量

------

### **它是怎么工作的？**

假设你有一个巨大的数据流，无法全部存储，比如包含几十亿个字符串或对象。

#### Step 1:

你对每个元素做哈希（使用一个 SUHA：Simple Uniform Hashing Assumption 的哈希函数），把它映射到 [0,1][0, 1] 区间上的一个随机实数。

#### Step 2:

你只记录 **哈希值最小的 kk 个值**（称为 *k-minimum values, KMV*）。这些值包含了有关集合“稀疏程度”的信息。

#### Step 3:

利用数学公式，如：

N≈kMkN \approx \frac{k}{M_k}

或

1N+1≈E[Mk]k\frac{1}{N+1} \approx \frac{\mathbb{E}[M_k]}{k}

来估算出集合的真实大小 NN。

------

### **为什么不直接数？**

在超大规模场景（比如亿级、百亿级元素）中：

- 直接存储所有元素太占空间
- 哈希后只保留极少数信息（前 kk 小值）就能进行有效估计
- 误差可控（误差与 1/k1/\sqrt{k} 成反比）

------

### **举个简单例子**

你有 10 亿个用户访问记录，想知道多少是“独立用户”。

用 Cardinality Sketch，你只哈希每个用户 ID，然后保留哈希值最小的前 k=1000k=1000 个，最后用公式反推出大概有多少独立用户 —— 只需 1000 个浮点数，远少于存储 10 亿个 ID。

------

### **它和 Bloom Filter 不同？**

是的：

| 项目 | Cardinality Sketch             | Bloom Filter                 |
| ---- | ------------------------------ | ---------------------------- |
| 目的 | **估计唯一元素个数**           | 判断某个元素是否**可能存在** |
| 类型 | 数值估计                       | 存在性测试                   |
| 空间 | 非常小（如只保存 kk 个哈希值） | 位数组（bitmap）             |

------

是否需要我用代码模拟一下？或者用图示补充这个思路？

![image-20250509195604257](./READEME (2).assets/image-20250509195604257.png)

这张图展示了 **Bloom Filter 在网站缓存（website caching）中的实际应用**，并强调了它在高性能系统中的价值。以下是详细解读：

------

## 一、应用场景：网站缓存策略

图中浏览器（Chrome 图标）在访问一个页面前会问：

> **“Loaded this before?”**

于是它用布隆过滤器判断页面是否**可能**已经被访问过。

------

## 二、两种情况：

### **1. 查询命中（可能访问过）**

- 图右侧提示：**“Cache this page!”**
- 表示：布隆过滤器判定该页面“可能已访问过”
- 实际处理策略：**缓存这个页面（通常是第二次访问时）**

手写说明：

- “Cache on 2nd visit”
- “Sometimes on first” → 即偶尔第一次访问时也缓存

### **2. 查询未命中（一定没访问过）**

- 图右侧蓝箭头：**“Add to filter (but don’t cache!)”**
- 表示：页面第一次被访问，先加入布隆过滤器，但**不立刻缓存**

------

## 三、布隆过滤器的作用：

### **优点：**

- **节省磁盘写入次数**
- 减少缓存写入负担，过滤掉只访问过一次的页面（不值得缓存）

图中下方折线图：

- y 轴：Disk writes per second（每秒磁盘写入次数）
- 横轴时间轴显示：布隆过滤器开启之后，磁盘写入负载明显下降
- 图中红字标注：“Bloom filter turned on” → 说明效果显著

------

## 四、误判不是问题！

图右上角红字说明：

> **“False positive is not a problem!”**

意思是：

- 就算 Bloom Filter 错误地认为“该页面访问过”（实际没访问过）
- 最多就是提前缓存了一次 → **缓存了几个额外页面不是坏事！**

手写备注补充：

- “Caching a few extra pages isn’t bad”

------

## 五、总结：这个策略的核心思路是

| 请求阶段 | Bloom Filter判断     | 动作             |
| -------- | -------------------- | ---------------- |
| 首次访问 | 不存在（肯定）       | 不缓存，只加入BF |
| 后续访问 | 可能存在（可能误判） | 缓存该页面       |

这种“缓存第二次访问的页面”的策略在 CDN、内容分发网络中非常常见。

------

## 六、引用来源：

右下角引用了论文：

> Maggs, Bruce M., and Ramesh K. Sitaraman.
>  *Algorithmic nuggets in content delivery*, ACM SIGCOMM CCR, 2015.

这说明这个策略是**经过业界实际应用验证的技术**。

------

如果你想，我可以帮你用这个思路写一个简易 Python 示例或伪代码，演示如何在缓存系统中集成 Bloom Filter。是否需要？

![image-20250509195959810](./READEME (2).assets/image-20250509195959810.png)

这张幻灯片讨论的是：**在 C++ 中如何实现一个 bit 向量（bit vector）**，也就是布隆过滤器使用的底层结构。以下是逐部分详细解释：

------

## 一、标题：Bitwise Operators in C++

说明我们关注的是 C++ 中用**按位操作（bitwise operators）**来构建和处理位向量。

------

## 二、问题：如何用 C++ 表示 bit vector？

> “**How can we encode a bit vector in C++?**”

也就是：在不浪费内存的前提下，如何构造一个 **m 位的数组（bit array）**。

------

## 三、常见方式分析

图中用红色与蓝色笔手写说明：

### **方法一：`vector<bool>`**

- 语法简洁，但有个隐藏问题：
  - `vector<bool>` 是 C++ 标准库中的**特化模板**，不是实际的向量容器
  - **不是线程安全的（can't be multi-threaded）**
  - **不能正确实现 vector 的特性**（如指针访问、引用赋值等）

图中蓝笔写着：

> ```
> vector<bool>` doesn't make vector
>  `→ can't be multi-threaded`
>  `→ doesn't implement parts of vector
> ```

------

### **方法二：用 `char` 或 `uint8_t` 表示每 8 个 bit**

- 1 byte = 8 bits，可将 bit vector 映射到一个 `char` 数组中
- 图中写着：
  - `char = 8 bits`
  - `uint8_t = 8 bits`
- 比如：
  - 你要查找第 3 位，就需要取出该字节，再用位掩码提取对应位置

底部红色二进制图展示了 bit 向量分布，每 8 个 bit 对应一个字节（char）

------

## 四、问题与挑战

图中蓝笔写着：

> “None are particularly good”
>  “Work w/ extra steps”

即：

- 无论是 `vector<bool>` 还是自己处理 `char[]`，都不理想
- 需要用**手动位操作**（bit shifts, masks）来模拟，增加复杂度

------

## 五、例子说明

图下方红笔演示了如何查找第 3 个 bit（从 0 开始）：

```
bit index:      0 1 2 [3] 4 5 6 7
bit values:     1 0 1 0 1 0 1 0
→ lookup entire char
```

要提取某个位，必须从整个字节中“掩码+移位”提取。比较繁琐。

------

## 六、小结：

| 方法                    | 优点           | 缺点/注意事项                       |
| ----------------------- | -------------- | ----------------------------------- |
| `vector<bool>`          | 语法简单，常用 | 特化类型，不是真 vector；线程不安全 |
| `char[]` 或 `uint8_t[]` | 可精确控制内存 | 需要手动做位操作，代码复杂          |

------

### **推荐：**

- 如果写布隆过滤器核心代码，**推荐用 `uint8_t[]` 或 `std::bitset`（若大小固定）**
- 结合按位操作 `|`, `&`, `~`, `<<`, `>>` 来处理每一位

------

是否需要我给你一个简单示例代码：比如用 C++ 实现 `setBit`, `getBit`, `clearBit` 操作函数？

![image-20250509203422518](./READEME (2).assets/image-20250509203422518.png)

这张幻灯片说明了 **C++ 中 bit vector 的位顺序读取习惯**，并指出了一个重要的**陷阱警告**：你在 Lab_Bloom 中读取位时不要按照“从右到左”的传统方式理解。以下是详细解析：

------

## 一、传统习惯：从右到左读取位

标题下方红字说明：

> **Traditionally, bit vectors are read from RIGHT to LEFT**

也就是说：二进制通常被解释为：

- **最低位在最右边**（bit 0）
- **最高位在最左边**（bit 7）

------

## 二、例子一（第一行 bit 向量）

位序列是：

```
0 0 0 0 1 1 1
```

红笔标记了每一位的权重（从右往左）：

```
64 32 16 8 4 2 1
```

非零位是 4、2、1，对应的值相加：

$1 + 2 + 4 = 7$

------

## 三、例子二（第二行）

位序列是：

```
1 0 0 0 1 0 1 0
```

从右到左依次赋权：

$1\cdot64 + 1\cdot8 + 1\cdot2 = 64 + 8 + 2 = 74$

------

## 四、核心警告：

> **Warning: Lab_Bloom won’t do this**

这意味着你在课程或实验 Lab_Bloom 中不应该**默认 bit 是从右往左解释的**！

### 可能原因：

- 自定义实现中 bit vector 是按 **顺序（从左到右）** 存储或读取的
- 也可能是因为 `std::vector<bool>` 或 `std::bitset` 实现中的特殊行为

------

## 五、总结：

| 项目             | 传统二进制表示       | Lab_Bloom 可能做法       |
| ---------------- | -------------------- | ------------------------ |
| 读取顺序         | 从右到左（低位在右） | 自定义方向（可能反过来） |
| 权重计算方式     | $2^0, 2^1, ...$      | 不一定按此顺序           |
| 编程时的注意事项 | 要小心位偏移、掩码   | 明确实现顺序逻辑         |

------

如果你想，我可以帮你写一个小函数来：
 1）展示如何从左/右方向解释 bit 序列
 2）或者帮你写 bit vector 访问器模板（适配 Lab_Bloom）

需要吗？

![image-20250509203608905](./READEME (2).assets/image-20250509203608905.png)

这张幻灯片是 **C++ 中常见按位运算符（bitwise operators）示例解析**，讲解非常清晰，并配有图示。下面是详细解读：

------

## **一、变量定义**

- `A = 10110`
- `B = 01110`

我们假设这两个是 5 位无符号二进制数，从左到右分别是高位（MSB）到低位（LSB）。

------

## **二、操作符解释与结果**

### 1. **按位取反（~）**

```
~B = 10001
```

说明：

- 将所有位**1 变 0，0 变 1**
- 注释：“invert all 1s and 0s”

------

### 2. **按位与（&）**

```
A & B = 00110
```

说明：

- 只有对应位 **都为 1，结果才是 1**，否则为 0
- 例：
  - A: `1 0 1 1 0`
  - B: `0 1 1 1 0`
  - 逐位与：`0 0 1 1 0`

注释：“Both have to be 1 for 1, else 0”

------

### 3. **按位或（|）**

```
A | B = 11110
```

说明：

- 只要有一个为 1，就输出 1
- 注释：“If either is 1, set 1, else 0”

------

### 4. **右移（A >> 2）**

```
A >> 2 = 00101
```

说明：

- 将 A 向右移两位（低位移出，高位补 0）
- 原始 A: `10110`
- 右移两位后：`00101`

注释：“discard least sig bits & replace most w/ 0”

------

### 5. **左移（B << 2）**

```
B << 2 = 11000
```

说明：

- 将 B 向左移两位（高位移出，低位补 0）
- 原始 B: `01110`
- 左移后：`11000`

注释：“discard most sig bits & replace least w/ 0”

------

## **三、图中蓝色小注释说明**

- 指出 **位 3 是否为 1**，与逻辑表达如 `B & 00100` 配合使用判断特定位状态
- `00100` 是一个掩码（mask）值，用来测试 B 的第 3 位（从右数第 3 位）

------

## **四、小结**

| 操作 | 名称     | 含义与用途                |
| ---- | -------- | ------------------------- |
| `~`  | 按位取反 | 所有位取反                |
| `&`  | 按位与   | 掩码测试、清零等          |
| `    | `        | 按位或                    |
| `>>` | 右移     | 除以 2 的幂，或向低位移动 |
| `<<` | 左移     | 乘以 2 的幂，或向高位移动 |

------

如果你想，我可以写一个简单的 C++ 程序，把这些操作都演示出来，并附带注释帮助你练习理解。需要吗？

![image-20250509203709737](./READEME (2).assets/image-20250509203709737.png)

这张幻灯片讨论的是布隆过滤器（Bloom Filter）在**数据集合合并（Union）或交集（Intersection）时的意义与优势**，特别强调了：

> **使用 Bloom Filter 合并（而不是直接合并数据集）可以极大提升效率与节省空间。**

以下是逐点详细解析：

------

## 一、问题导向：

> **What is the conceptual meaning behind union and intersection?**

我们要理解的是：两个数据集的合并（或交集）在 bit vector 层面上是如何表示的。

------

## 二、例子结构（图解说明）

### 左边：两个数据集

- `Dataset A` 和 `Dataset B` 分别表示两个原始集合
- 使用相同的哈希函数将其分别构建成两个 Bloom Filter（BF）
  - BF_A
  - BF_B
- 图中写着：“**have same hash function**” 是合并的必要条件

------

## 三、合并方式对比：

### 方法一：**直接合并原始数据集**

- 即：`C = A ∪ B`
- 然后对 `C` 再建一个新的 Bloom Filter

图中红字评论：

- **“union datasets”**
- **“slow!”**
- **“Big!”**
- **“Bad!”**

这是最慢也最浪费空间的方法，因为原始数据集通常很大。

------

### 方法二：**合并 Bloom Filter 本身**

- 利用位操作 `bitwise OR`：

  BFC=BFA ∣ BFB\text{BF}_C = \text{BF}_A \, | \, \text{BF}_B

- 每个位：只要 A 或 B 的某个位为 1，合并后的该位就是 1

图中写着：

- **“union BF”**
- **“fast”**
- **“small”**
- **“good”**
- 最终：“same filter at end!”

------

## 四、交集（intersection）也可以实现：

虽然图中主讲 union，但实际上：

- 交集可通过 `bitwise AND`：

  BFC=BFA & BFB\text{BF}_C = \text{BF}_A \, \& \, \text{BF}_B

- 可用于**快速排除非共有元素**

------

## 五、总结：

| 方法              | 操作对象      | 优点         | 缺点               |
| ----------------- | ------------- | ------------ | ------------------ |
| 合并原始数据集    | Dataset A ∪ B | 精确         | 慢、耗内存         |
| 合并 Bloom Filter | BF_A \| BF_B  | 快、占内存小 | 存在误判（可容忍） |

------

这个技巧在搜索引擎、数据库索引、网络黑名单合并中非常实用。

如果你想，我可以举个具体例子（两个集合如何转化成 BF 并合并），或提供 Python 演示代码。需要吗？

![image-20250509203859373](./READEME (2).assets/image-20250509203859373.png)

这张幻灯片是引出 **Sequence Bloom Trees (SBT)** 的动机，用于解决在**大量生物序列文件中高效搜索目标序列**的问题。我们逐一解析：

------

## 一、背景场景

> **“Imagine we have a large collection of text…”**

- 这些文本文件包含的是类似 DNA 的字符串（如：`ATGTTGAATTAACCCGG...`）
- 每个文件中可能包含数千到数百万条生物序列

------

## 二、目标

> **“And our goal is to search these files for a query of interest…”**

也就是说：

- 我们想检查一个**查询字符串是否存在于这些文件中**
- 查询可能是某一段 DNA 子串，比如：“ACCGGTTAA”

------

## 三、问题挑战

如果你逐文件暴力搜索（brute-force）：

- 成本高（每次都要加载文件）
- 慢（尤其是海量数据时）

------

## 四、动机：用布隆过滤器加速查询

幻灯片中绿色的文件表示：

- 使用某种结构提前为每个文件构建索引
- 只搜索可能命中的文件
- 跳过一定“不可能命中”的文件（即 Bloom Filter 判断为不存在）

这就引出了接下来的 **SBT**：

------

## 五、Sequence Bloom Tree 的核心思想（预告）

- 为每个文件构建一个布隆过滤器，表示它可能包含的 k-mer（固定长度子串）
- 然后把这些 Bloom Filters 组织成树状结构
- 查询时只需走**可能匹配路径**

------

## 六、总结：这页幻灯片的逻辑链

| 元素             | 含义                                              |
| ---------------- | ------------------------------------------------- |
| 一堆文件         | 每个是一个生物数据集                              |
| 文件中的绿色文本 | DNA 序列，后面会被切成 k-mer 存入 Bloom Filter    |
| 搜索目标         | 想找某个子串是否存在于某些文件                    |
| 多个绿色文件     | 说明它们是**可能匹配的文件**，提前被标记/筛选出来 |

------

如果你想，我可以帮你进一步解释接下来的 SBT 是怎么构建与查询的，或者帮你写个模拟 SBT 查询流程的代码。要继续吗？

![image-20250509210916506](./READEME (2).assets/image-20250509210916506.png)

这张图解释的是**基数估计（Cardinality Estimation）**的一个基本思想，用于估计集合中不重复元素的个数。

我们来逐行详细解释：

------

### 【标题】

**Cardinality Estimation**：
 基数估计，即估算某个集合中有多少个**唯一元素**（distinct elements），是大数据处理中常用的技术。

------

### 【正文】

**Imagine we have a SUHA hash h over a range m.**
 假设我们有一个 **SUHA 哈希函数** `h`，它将元素映射到区间 `[0, m-1]` 上。

- SUHA 全称是 **Simple Uniform Hashing Assumption（简单均匀哈希假设）**，意思是：每个输入值 `x` 被哈希函数 `h(x)` 映射到 `[0, m-1]` 之间是等概率的。

------

**Inserting a new key is equivalent to adding a card to our hat!**
 插入一个新元素，相当于往“帽子”里加一张牌。

- 这是一个比喻：我们往哈希函数中输入键（key），就像往帽子里放卡片。

------

**Tracking only the minimum value is a sketch that estimates the cardinality!**
 只追踪**最小的哈希值**，就可以作为一种“sketch”来估计基数。

- **Sketch**：一种空间高效的数据摘要结构。
- 原理：如果你把许多元素哈希到 `[0, m-1]`，并只记录最小的哈希值 `min(h(x))`，那么根据概率统计知识，就可以用这个最小值反推出大致有多少不同的元素。

------

### 【下方图示】

图中画了一个三角形，表示哈希值的范围：

- 左端是 `0`，右端是 `m - 1`，顶部是某个 `h(x)` 值。
- 灰色区域表示可能的哈希结果，蓝色底部区域强调**我们关注的只是最小值**。

------

### 【总结】

这张图讲述了基于 SUHA 哈希、只记录哈希最小值（或最大值、稀疏值）来进行**基数估计**的基本思路。这种方法属于**概率算法**，代表性的算法有：

- **MinCount**
- **LogLog**
- **HyperLogLog**

它们可以在 **不存储全部数据的情况下**，快速且节省内存地估计集合的大小。

------

需要我补充数学公式推导或者举个实际例子说明吗？

![image-20250509211515810](./READEME (2).assets/image-20250509211515810.png)

这张幻灯片继续讲解**基数估计（Cardinality Estimation）**，在上一张图的基础上进一步引入了“归一化哈希”的概念。以下是逐句和图示的详细中文解释：

------

### 标题：Cardinality Estimation（基数估计）

------

### 第一部分（重复前面内容，便于上下文连贯）

- **假设我们有一个 SUHA 哈希函数 h，它的输出范围是 [0, m-1]。**
- **每插入一个新的 key，相当于往帽子里丢一张卡片。**
- **如果我们只追踪这些哈希值中的最小值，它就构成了一个 sketch，可用于估计元素数量。**

------

### 新内容部分：

#### “To make the math work out, let’s normalize our hash…”

“为了便于数学推导，让我们对哈希值做归一化处理……”

也就是说，我们将原始的哈希值 `h(x)` 映射到 **[0,1]** 之间，而不是 `[0, m-1]`。这样做的好处是：

- 数学上更方便处理（概率密度、分布函数等在区间 [0,1] 上有标准形式）；
- 更利于后续使用数学模型推导估计公式（例如 `E[min] = 1/(n+1)` 等）。

------

### 公式解释：

#### `h′(x) = h(x) / (m - 1)`

这是归一化公式，将哈希值从区间 `[0, m-1]` 映射到 `[0, 1]`，即：

- `h(x)` 是原始哈希值；
- `m` 是哈希函数的输出范围长度；
- `h′(x)` 是归一化后的值。

这个公式确保：

- 如果 `h(x) = 0`，那么 `h′(x) = 0`；
- 如果 `h(x) = m - 1`，那么 `h′(x) = 1`；
- 所以 `h′(x)` 的值总在 `[0,1]` 区间内。

------

### 图示解释：

图中显示的是一个从 `0` 到 `1` 的灰色三角形区域，对应 `h′(x)` 的值域。

- 顶部的 `h′(x)` 显示这是我们计算/关注的值；
- 横轴是归一化的哈希区间 `[0, 1]`；
- 蓝色底部强调仍然是**追踪最小值**。

------

### 总结：

这张幻灯片的关键点是：**为了后续更简洁地推导数学公式，我们将哈希值归一化到 [0,1] 区间。**

这个操作在很多基数估计算法（如 MinCount、HyperLogLog）中是非常常见的步骤，有助于构建与概率分布相关的估计方法。

------

需要我进一步说明归一化后如何用最小值估计集合大小吗？或者推导它的期望和估计公式？

![image-20250509220457436](READEME%20(2).assets/image-20250509220457436.png)

这张幻灯片解释的是 **Cardinality Estimation（基数估计）** 中最基本的思想之一：**最小值的概率分布与样本个数之间的关系**，这为 **KMV (K Minimum Values) sketch** 和 **MinHash** 等技术打基础。

下面是逐行详细解释：

------

### 一、问题设定

> **Consider an N+1N+1 draw**

假设我们从 [0,1][0,1] 区间内均匀独立抽取了 N+1N+1 个数：

- X1,X2,...,XNX_1, X_2, ..., X_N：已有的 N 个数
- XN+1X_{N+1}：新加入的第 N+1N+1 个数

我们关注的是：

> XN+1X_{N+1} 是否会成为新的最小值？

------

### 二、最小值的概率逻辑

定义：

- M=min⁡1≤i≤NXiM = \min\limits_{1 \leq i \leq N} X_i：当前 N 个数的最小值

如下图所示，我们将 [0,1][0,1] 区间分成两段：

1. [0,M)[0, M)：新数如果落在这个区间，它会**小于原来的最小值**，从而成为新的最小值。
2. [M,1][M, 1]：新数不会改变最小值

因此：

- XN+1X_{N+1} 成为新最小值的概率 = **MM**
- XN+1X_{N+1} 保持当前最小值的概率 = **1−M1 - M**

------

### 三、为什么这很重要？

在很多基数估计算法中（比如 KMV Sketch）：

- 我们不是存所有的元素，而是存他们哈希值的最小值（或前 K 小）
- 如果你插入一个新元素，它是否能“影响”最小值，是有概率控制的
- 这个概率可以间接告诉我们插入了多少元素（**反推出 n 的估计值**）

------

### 四、图形说明（底部进度条）

横轴是区间 [0,1][0,1]，灰色部分是：

- **落在 [0,M)[0, M)** 区域，新值会更新最小值（概率 = MM）

蓝色部分是：

- **落在 [M,1][M, 1]** 区域，最小值不变（概率 = 1−M1 - M）

------

### 五、总结关键点

| 项目                       | 含义                                   |
| -------------------------- | -------------------------------------- |
| M=min⁡XiM = \min X_i        | 当前最小哈希值                         |
| XN+1<MX_{N+1} < M 的概率   | 就是 MM                                |
| XN+1≥MX_{N+1} \ge M 的概率 | 就是 1−M1 - M                          |
| 应用意义                   | 用最小哈希值反推插入元素的数量（基数） |

------

如果你想，我可以进一步演示这个原理是如何被用在 KMV Sketch 或 HyperLogLog 中进行基数估计的。需要吗？

![image-20250509222816262](READEME%20(2).assets/image-20250509222816262.png)

这张幻灯片继续讲的是 **Cardinality Sketch** 中最小值（Minimum）的期望值估计 —— 是理解 **KMV Sketch**（K-Minimum Values）原理的关键。

我们逐行详细解释：

------

## 一、场景设定（延续上一页）

- 有 NN 个样本：X1,X2,...,XNX_1, X_2, ..., X_N，从 [0,1][0,1] 中**均匀独立抽样**

- 我们定义：

  M=min⁡1≤i≤NXiM = \min_{1 \le i \le N} X_i

- 现在新增一个样本 XN+1X_{N+1}，我们关注它是否会变成新的最小值

------

## 二、关键结论

> **“XN+1X_{N+1} will be the new minimum with probability MM”**

这是从上一页推导出的结论 —— 如果你在 [0,1][0,1] 区间上均匀采样，那么：

- XN+1X_{N+1} 落在 [0,M)[0, M) 的概率 = MM，此时它变成最小值

------

## 三、引入 SUHA 假设

> **By definition of SUHA, XN+1X_{N+1} has a 1N+1\frac{1}{N+1} chance of being the smallest item**

SUHA 是 **Simple Uniform Hashing Assumption**（简单均匀哈希假设）：

- 所有样本都是独立、均匀地从 [0,1][0,1] 区间选出来的
- 因此，N+1N+1 个独立样本中，每一个都有相同概率成为最小值，概率为：

1N+1\frac{1}{N+1}

------

## 四、推出期望公式：

> **E[M]=1N+1\mathbb{E}[M] = \frac{1}{N+1}**

解释：

- 对于从 [0,1][0,1] 中独立采样的 NN 个随机变量
- 它们的最小值 M=min⁡XiM = \min X_i 的**期望值**为：

E[M]=∫01x⋅fM(x) dx=1N+1\mathbb{E}[M] = \int_0^1 x \cdot f_M(x)\,dx = \frac{1}{N+1}

这是经典统计结论，可从 order statistics 推出，也可以用概率积分法证明。

------

## 五、底部图像说明

- 横轴是 [0,1][0,1] 区间
- MM 在灰色区域中
- 灰色区表示：“XN+1X_{N+1} 成为新最小值”的概率 = MM
- 蓝色区表示：“最小值不变”的概率 = 1−M1 - M

------

## 六、应用意义

这个结论在 **KMV / MinCount** 算法中非常重要：

- 如果你记录了 N 个哈希值中最小的一个 MM

- 那你可以用：

  N^≈1M−1\hat{N} \approx \frac{1}{M} - 1

  来估计原始元素数量

------

需要我接着推导这条估计公式，或带你实现一个 KMV 小示例吗？

![image-20250509222940124](READEME%20(2).assets/image-20250509222940124.png)

这张幻灯片展示了如何利用“最小值抽样（Min-Value Sampling）”来估计基数（cardinality，即元素个数）的方法，并通过三次实际抽样尝试（Attempt）演示了这种估算的过程。

------

### 一、核心公式复习

> - **结论**：

E[M]=1N+1⇒N≈1M−1\mathbb{E}[M] = \frac{1}{N+1} \quad \Rightarrow \quad N \approx \frac{1}{M} - 1

- MM：从 NN 个哈希值中选出的最小值（min）
- 利用 MM 可以反推估计原始集合的大小 NN

------

### 二、三个抽样尝试解释

每一行都有 5 个在 [0,1][0,1] 上均匀采样的哈希值。我们从中取出最小值 MM，然后根据公式估算 NN。

------

#### **Attempt 1**：

数据为：

```
0.962, 0.328, 0.771, 0.952, 0.923
```

- 最小值：M=0.328M = 0.328

- 估计大小：

  N^=10.328−1≈3.05−1=2.05\hat{N} = \frac{1}{0.328} - 1 \approx 3.05 - 1 = 2.05

------

#### **Attempt 2**：

数据为：

```
0.253, 0.839, 0.327, 0.655, 0.491
```

- 最小值：M=0.253M = 0.253

- 估计大小：

  N^=10.253−1≈3.95\hat{N} = \frac{1}{0.253} - 1 \approx 3.95

------

#### **Attempt 3**：

数据为：

```
0.134, 0.580, 0.364, 0.743, 0.931
```

- 最小值：M=0.134M = 0.134

- 估计大小：

  N^=10.134−1≈6.46\hat{N} = \frac{1}{0.134} - 1 \approx 6.46

------

### 三、观察与启示

- 真正的 N=5N = 5，因为每次都是从 5 个值中抽样
- 三次估计结果分别为：
  - 2.05（低估）
  - 3.95（低估）
  - 6.46（略高估）

> 说明：单次抽样波动较大，但**多次尝试取平均值可提高准确性**

------

### 四、实际应用扩展

- KMV Sketch 就是记录前 kk 小的哈希值，而不是最小的一个

- 然后使用类似的估计方法，例如：

  N^=kX(k)\hat{N} = \frac{k}{X_{(k)}}

  其中 X(k)X_{(k)} 是第 kk 小的哈希值

------

需要我帮你把这种估计方式代码实现一下（Python/C++/伪代码）吗？还是要继续看下一张图？

这是概率统计中的一个经典推导，用于说明**如何利用最小值 MM 来估计集合大小 NN**。下面是详细推导步骤：

------

## 背景设定

我们从 NN 个不同元素中各自计算一个哈希值，哈希函数将每个元素**均匀地映射到 [0,1][0, 1]** 区间。

设这 NN 个哈希值为 X1,X2,…,XNX_1, X_2, \ldots, X_N。

我们关注：

- 这 NN 个值的最小值 M=min⁡(X1,…,XN)M = \min(X_1, \ldots, X_N)
- 想估计原始集合大小 NN

------

## 一、期望推导：E[M]=1N+1\mathbb{E}[M] = \frac{1}{N+1}

![image-20250509223154267](READEME%20(2).assets/image-20250509223154267.png)

------

## 三、实际意义

这个方法背后其实是在用**哈希值的稀疏程度**来反推**去重集合中元素的个数**。

比如：

- 如果最小值是 0.05，说明哈希值很密，说明集合很大
- 如果最小值是 0.8，说明哈希值很稀，说明集合很小

------

是否需要我推导积分部分，或者配合 Python 示例进一步演示？

![image-20250509223228134](READEME%20(2).assets/image-20250509223228134.png)

这张图解释了在**基数估计（cardinality estimation）**中，使用第 kk 小哈希值 MkM_k 来估计集合大小 NN 的改进方法。以下是详细解释：

------

### 一、原来的方法（复习）

我们知道，若对集合中的元素使用均匀哈希到 [0,1][0, 1]，然后取最小值 M1M_1，那么有：

![image-20250509223429447](READEME%20(2).assets/image-20250509223429447.png)

这个方法的**缺点**是：仅仅用了最小值，容易受噪声影响（variance 较大）。

------

### 二、改进思路：使用第 kk 小值 MkM_k

图中提出一个更鲁棒的做法：

> 不用最小值 M1M_1，而用第 kk 小的哈希值 MkM_k 来估计 NN

这个方法叫做 **KMV Sketch（K Minimum Values Sketch）**。

------

### 三、核心公式与含义

图中主张：

![image-20250509223420465](READEME%20(2).assets/image-20250509223420465.png)

也就是说，第 kk 小的哈希值（不是最小值）**在期望上是 k/(N+1)k/(N+1)**，这比使用最小值 M1M_1 更稳定。

------

### 四、为什么准确度更高？

图中分解了这个平均值：

![image-20250509223413703](READEME%20(2).assets/image-20250509223413703.png)

你可以理解为：

> 把第 kk 小值的期望看作一系列「增量」叠加起来的平均：
>  第一个最小值 + 第二个比第一个多了一点 + 第三个再多一点 + …，最后平均一下。

图底部的横线就是表示哈希值从 0 到 1，按从小到大划分出 M1,M2,…,MkM_1, M_2, \dots, M_k。

------

### 五、实际应用中怎么做？

1. 对集合中所有元素哈希成 [0,1][0,1]

2. 取前 kk 小的哈希值

3. 令 Mk=M_k = 第 kk 小者

4. 用公式估计集合大小：

   ![image-20250509223402988](READEME%20(2).assets/image-20250509223402988.png)

------

### 总结对比

| 方法     | 用到的哈希值 | 估计公式                           | 优缺点           |
| -------- | ------------ | ---------------------------------- | ---------------- |
| 最小值法 | 只用 M1M_1   | N≈1M1−1N \approx \frac{1}{M_1} - 1 | 简单但不稳定     |
| KMV法    | 用前 kk 小   | N≈kMk−1N \approx \frac{k}{M_k} - 1 | 稳定，适合大数据 |

是否需要我用 Python 演示一下模拟这两个方法的效果差异？

![image-20250509223500478](READEME%20(2).assets/image-20250509223500478.png)

这张图是对上一张图的进一步解释，说明了**为什么用第 kk 小最小值 MkM_k 来估计集合大小 NN 更稳定、更准确**。我们一起来逐步解读：

------

## 一、核心公式回顾

图上写的是：

![image-20250509223615789](READEME%20(2).assets/image-20250509223615789.png)

也就是说，**第 kk 小最小值的期望 MkM_k** 和集合大小 NN 存在一个反比关系，类似之前的 ![image-20250509223622512](READEME%20(2).assets/image-20250509223622512.png)

------

## 二、右边的分解含义

图继续把 E[Mk]\mathbb{E}[M_k] 分解为：

![image-20250509223633431](READEME%20(2).assets/image-20250509223633431.png)

然后两边除以 kk，变成：

![image-20250509223642427](READEME%20(2).assets/image-20250509223642427.png)

你可以理解为：**将哈希空间 [0,1] 划分成 kk 个“桶”或“区间”**，每一段估计一个「最小值」，最后**平均一下**，得出比只看一个点（如最小值）更准确的期望。

------

## 三、底下的图形解释

图的下方画了一个 [0,1][0,1] 区间，按从左到右标出了：

- M1M_1、M2M_2、M3M_3、...、MkM_k
- 每个竖线表示一个哈希值的分布位置
- 每两个相邻 MiM_i 之间的间距，反映了**哈希值稀疏程度**
- 蓝色区域表示整体分布

你可以理解为：

- 越小的 MiM_i，出现的可能性越大
- 我们用多个 MiM_i 的期望，来平均出更稳定的估计值（降低方差）

------

## 四、最后总结

| 项目     | 内容                                                       |
| -------- | ---------------------------------------------------------- |
| 原始方法 | 只用 M1M_1                                                 |
| 改进方法 | 用第 kk 小值 MkM_k                                         |
| 公式     | E[Mk]k≈1N+1\frac{\mathbb{E}[M_k]}{k} \approx \frac{1}{N+1} |
| 优势     | 通过 **k 次估计平均**，减小误差，提高稳定性                |
| 图形意义 | 将 [0,1][0,1] 区间按哈希值等分，估计更稳定                 |

------

这就是 KMV（K Minimum Values）技术的数学核心：**采样多个最小哈希值，估计去重元素数量（cardinality）**。需要我接下来用代码或图再演示一下估计效果吗？

![image-20250509223905197](READEME%20(2).assets/image-20250509223905197.png)

这张图是对 **Cardinality Sketch（基数估计）** 的一个可视化引导，展示了如何通过哈希值的 **前 kk 个最小值（k-th minima）** 来估计集合中不重复元素的数量（即集合的基数）。我们来详细解释这张图的含义。

------

## 一、背景：想估计集合的基数（unique items 数量）

图中左侧表示一个包含大量数据的集合，比如：

- 生物序列（如 FASTQ 文件中的 DNA 读段）
- 图像数据（如猫图）
- 图结构（如社交网络图）

目标是**估计其中有多少个唯一的对象**，而不是存下所有对象。

------

## 二、核心方法：哈希 + 取前 kk 小值

图中间的黑色箭头代表对这些对象应用了一个哈希函数（SUHA：Simple Uniform Hashing Assumption），将每个元素映射为 [0,1][0, 1] 区间上的随机数。例如：

```
ATGGTTAGAATTAACC...  →  0.253
CGATAGCACAGGTAGT...  →  0.839
TACGTAGAGGT...       →  0.327
```

这些值保存在右侧的蓝色方块中。

------

## 三、重点：用最小的 kk 个哈希值估计基数

> To use the *k*-th min, we have to track *k* minima.

要想使用第 kk 小哈希值（即最小的 kk 个 hash 值）来估计集合大小，就必须实时维护一个最小堆，保存前 kk 小的哈希结果。

根据前面提到的公式：

![image-20250509224154452](READEME%20(2).assets/image-20250509224154452.png)

其中 MkM_k 是第 kk 小哈希值。

------

## 四、问题引出：能不能用 **所有前 kk 小值**？

图底部提出了一个关键问题：

> Can we use **ALL** minima?

也就是说，**我们能不能不仅仅用第 kk 小值，而是用所有前 kk 小值的平均来提高估计稳定性？**

答案是**可以的**，这就是后续你看到的那种：

![image-20250509224132213](READEME%20(2).assets/image-20250509224132213.png)

形式的公式背后的动机 —— 多个最小值的平均比单个值更稳定、更抗噪声。

------

## 总结

| 元素            | 含义                                                         |
| --------------- | ------------------------------------------------------------ |
| SUHA 哈希       | 把所有对象映射为 [0,1][0,1] 之间的浮点数                     |
| kk 个最小哈希值 | 跟踪集合的稀疏性（估算 unique 数量）                         |
| 估计公式        | ![image-20250509224143612](READEME%20(2).assets/image-20250509224143612.png) |
| 结论            | 不止可以用第 kk 小值，还可以用前 kk 小值的**平均**来提高估计效果 |

要不要我用简单代码演示一下这个估计过程？

![image-20250509224726701](READEME%20(2).assets/image-20250509224726701.png)

这张图展示了**Similarity Sketches（相似度草图）\**的核心思想，它基于 SUHA（简单均匀哈希假设）来估计两个集合的\**相似度**，尤其是**Jaccard 相似度**。

------

### **图中表达的核心思想是：**

> 在哈希均匀的前提下，两个集合的相似度 ≈ 它们前 kk 个最小哈希值中，**重合的比例**。

------

### **图中元素含义逐一解释：**

- 蓝色长条代表两个集合 A 和 B。
- 每个集合的成员经过哈希函数后得到一个 [0, 1] 的实数。
- 所有哈希值从小到大排序，选出最小的 kk 个值（即 KMV：k-minimum values）。
- 图中红框标出了前 kk 个最小哈希值的位置（比如左边红框中的4个）。
- **黑色箭头**表示某个集合的哈希值中选中了这个位置（出现了）。
- **紫色三角表示“命中”**（即某个集合中有这个哈希值）

------

![image-20250509224845453](READEME%20(2).assets/image-20250509224845453.png)

------

### **总结：Similarity Sketch 是干嘛的？**

它是 Cardinality Sketch 的延伸，用于估计：

> “两个集合有多相似？”

而且只需比较**极少数哈希值的匹配情况**，即可高效估计两个集合之间的相似度，常用于：

- 基因组相似性计算（如图）
- 大规模文本比对（文档去重）
- 日志流相似性判断
- 数据库中的集合比较

是否需要我用代码或图再补充“如何选出 k 个最小哈希”或“Jaccard 相似度原理”？

![image-20250509224926114](READEME%20(2).assets/image-20250509224926114.png)

这张图解释了 **MinHash Sketch** 与 **Cardinality Sketch** 的关系，以及它们的区别与扩展。

------

## 一、核心概念解释：

### 1. **Cardinality Sketch（基数草图）**

- 目的是估计集合中有多少**不同的元素（unique items）**。

- 方法是跟踪哈希后最小的 kk 个值（称为 **k-minimum values**）。

- 只使用这些哈希值中的**第 kk 小值（第 kk 个最小值）**来估计总数 NN，通过公式：

  ![image-20250509225049862](READEME%20(2).assets/image-20250509225049862.png)

- 比如图中左侧蓝色方块堆叠就是一个集合，它有多个元素，但我们只保留最小的 kk 个哈希值，推算“有多少元素”。

------

### 2. **MinHash Sketch（最小哈希草图）**

- 用来估计**集合之间的相似性**（Jaccard 相似度）。
- 核心思想是：不同集合通过同一个哈希函数哈希后，保留前 kk 小的哈希值，然后比较这 kk 个值的重合程度。
- 比如图中右边红、绿两堆代表两个集合，它们的重叠矩形块越多，说明这两个集合越相似。

------

## 二、图中内容解析：

### 图上半部分文字：

- `k-th minimum value sketch` 是指**只用一个最小哈希值来估计 cardinality**。
- 而 `MinHash sketch` 是对这种方法的**扩展**，使用 **全部前 kk 个最小哈希值**来比较集合之间的重合，从而**估计相似度**。

------

### 图下半部分图示：

- 左图（蓝色 + 黑色块）：表示 tracking 最小 kk 个哈希值（用来估计 cardinality）。

- 右图（红绿交叉）：表示两个集合的哈希结果中保留了各自的 kk 个最小值，其中**重合部分（交叉块）**用于估计 Jaccard 相似度：

  ![image-20250509225040733](READEME%20(2).assets/image-20250509225040733.png)

------

## 三、总结一句话：

- **Cardinality Sketch** 是用“最小哈希值”估计集合有多大；
- **MinHash Sketch** 是用“最小哈希值集合”估计集合之间有多像；
- 本图就是在强调 **从估计“多少个”拓展到“多相似”**。

是否需要我用一组具体数据来模拟 MinHash 相似度的计算？

![image-20250509225104611](READEME%20(2).assets/image-20250509225104611.png)

这张图展示了 **如何构造一个 MinHash Sketch**（最小哈希草图）——用于快速估算集合间的相似度。图中列出了构造它所需要的三个关键输入，并配有直观的图示。

------

## 一、什么是 MinHash Sketch？

MinHash Sketch 是一种压缩表示集合的方法，它通过保留集合中**经过哈希后的最小值**（或最小的前 kk 个值）来估算集合间的 **Jaccard 相似度**。

------

## 二、构造 MinHash Sketch 需要三要素（图中列出）：

### 1. **一个可哈希的数据集**

- 即集合 AA，可以是网页集合、基因片段、用户行为等。
- 图中蓝帽子代表某个集合（比如一个文档或基因集合），右边浅色方块是它的元素。

------

### 2. **一个哈希函数（hash function）**

- 用于将集合中的每个元素变换为一个[0, 1]之间的随机数。
- 要求哈希函数满足 SUHA（Simple Uniform Hashing Assumption），即对元素均匀分布。
- 图中注释：**This is hard!** ——说明设计一个高质量哈希函数并不简单。

------

### 3. **选择 kk：需要保留多少个最小值**

- 我们不会保留所有哈希值，而是只保存**前 kk 个最小哈希值**。
- kk 越大，结果越准确，但存储和计算成本也越高。
- 图中最上方“how many values I need”即表示这个意思。

------

## 三、图示解读：

- 图右侧蓝色方块表示从集合中哈希出来的值。
- 下方的堆叠方块就是“MinHash Sketch”本体：只保留了最小的 kk 个哈希值（也就是你最后拿来比较集合相似度的“签名”）。
- 也就是说，**原始数据被“压缩”为这 kk 个哈希值**。

------

## 四、MinHash Sketch 用来干嘛？

- 拿两个集合的 MinHash Sketch 进行比对，估计它们的 **Jaccard 相似度**：

  ![image-20250509225159941](READEME%20(2).assets/image-20250509225159941.png)

- **高效**、**省空间**，特别适合大规模集合比对，例如网页去重、基因相似分析、推荐系统等。

------

需要我进一步演示如何用两个集合的 MinHash 估计相似度吗？

![image-20250509225211001](READEME%20(2).assets/image-20250509225211001.png)

这张图是对 **MinHash 构造过程**的一个非常具体的例子，用来展示 MinHash Sketch 是如何构造出来的。

------

## 一、问题设定：

你有一个集合：

S={16,8,4,13,15}S = \{16, 8, 4, 13, 15\}

定义了一个简单的哈希函数：

h(x)=x%7h(x) = x \% 7

你希望构造一个 **MinHash sketch**，只保留最小的 **k = 3** 个哈希值。

------

## 二、具体计算：

对集合中的每个元素，代入哈希函数：

- h(16)=16%7=2h(16) = 16 \% 7 = 2
- h(8)=8%7=1h(8) = 8 \% 7 = 1
- h(4)=4%7=4h(4) = 4 \% 7 = 4
- h(13)=13%7=6h(13) = 13 \% 7 = 6
- h(15)=15%7=1h(15) = 15 \% 7 = 1

现在你得到的哈希值集合是：

{2,1,4,6,1}\{2, 1, 4, 6, 1\}

------

## 三、保留最小的 k=3k=3 个值：

从上面的哈希值中取最小的 3 个（去重）：

- 1（出现两次）
- 2
- 4

最终的 MinHash Sketch 就是：

{1,2,4}\{1, 2, 4\}

图右侧的表格即表示保存的这三个哈希值，分别放在索引 0, 1, 2 处。

------

## 四、补充说明：

- **为什么忽略重复？**
   因为 MinHash 只关心哈希值的“出现”，不关心频率。
- **为什么只取最小的 kk 个？**
   因为这些最小哈希值能以概率的方式代表集合的特征，从而可用于估算集合之间的相似度（例如 Jaccard 相似度）。

------

需要我再举个两个集合做 Jaccard 估计的例子，说明 MinHash 如何用于比对两个集合的相似度吗？

![image-20250509225348990](READEME%20(2).assets/image-20250509225348990.png)

这张图展示的是如何使用 **MinHash** 来估计两个集合（A 和 B）之间的 **Jaccard 相似度**。

------

### 【背景知识】

Jaccard 相似度定义为：

Jaccard(A,B)=∣A∩B∣∣A∪B∣\text{Jaccard}(A, B) = \frac{|A \cap B|}{|A \cup B|}

MinHash 提供了一个 **压缩估计**：我们不存储所有元素，而是只记录哈希值最小的前 kk 个（称为 **sketch**），通过比较两个 sketch 的重合程度来近似计算 Jaccard 相似度。

------

### 【图中内容解释】

#### **Sketch A：**

- 哈希最小的 8 个值为：`{3, 7, 8, 11, 15, 17, 22, 23}`（图中红色）

#### **Sketch B：**

- 哈希最小的 8 个值为：`{2, 3, 6, 7, 9, 11, 17, 23}`（图中蓝色）

------

### 【估计 Jaccard 相似度】

1. **找出两个 sketch 中都出现的值（交集）**：

   共同的值={3,7,11,17,23}⇒∣A∩B∣=5\text{共同的值} = \{3, 7, 11, 17, 23\} \Rightarrow |A \cap B| = 5

2. **总共看了多少对值（并集近似）？**
    两个 sketch 都是长度 8，所以一共看了 8 个位置（按位置比较）。

   ⇒∣A∪B∣≈8\Rightarrow |A \cup B| \approx 8

3. **Jaccard 估计值**：

   估计相似度=58=0.625\text{估计相似度} = \frac{5}{8} = 0.625

------

### 【图下方时间线表示】

图下方的横线是一个**哈希值轴**（横坐标），展示了在原始全集里，A 和 B 中哪些哈希值最小。每个值都按从小到大的顺序排列，用于直观地看出它们的重合情况。

------

### 【结论】

这张图的目的是说明：

- 我们不需要保存完整集合，就能用少量哈希值的 **重合比例** 来估计 Jaccard 相似度；
- MinHash 的核心思想是用 **哈希函数压缩集合**；
- 重合得越多，两个集合越相似。

------

要不要我给你用 Python 举个简单例子来验证 Jaccard 估计的准确性？

![image-20250509225503039](READEME%20(2).assets/image-20250509225503039.png)

这张图是对 **MinHash Jaccard Estimation**（使用 MinHash 估计集合相似度）的一步延伸，解释了如何从两个集合的 MinHash sketch 推断出它们的并集（A ∪ B）的 sketch。

------

### **背景回顾**

- **MinHash Sketch**：为每个集合记录哈希值最小的前 kk 个值（bottom-k hash values），作为该集合的代表。

- **Jaccard 相似度**：

  J(A,B)=∣A∩B∣∣A∪B∣J(A, B) = \frac{|A \cap B|}{|A \cup B|}

  使用 MinHash，可以通过比较两个 sketch 中有多少个元素是一样的来估计这个比例。

------

### **这张图的含义**

图中展示了：

- 左边红框是集合 A 的 MinHash Sketch（红色数字）；
- 中间蓝框是集合 B 的 MinHash Sketch（蓝色数字）；
- 右边紫框是 A ∪ B 的估计 sketch（蓝蓝红混合）；
- 下方是哈希值时间线，展示 A 和 B 中最小值的位置（用于可视化重合）。

------

### **红圈中 A ∪ B 的 sketch 如何来的？**

我们取 A 和 B 的 MinHash 值的 **并集**，然后从中取前 8 小（bottom-8）：

```text
A: 3, 7, 8, 11, 15, 17, 22, 23
B: 2, 3, 6, 7, 9, 11, 17, 23
A ∪ B: 去重后是：2, 3, 6, 7, 8, 9, 11, 15, 17, 22, 23
取前 8 小 = {2, 3, 6, 7, 8, 9, 11, 15}
```

右侧的表格就是这个结果，表示的是：

> 如果你真的构建了 A ∪ B 的 sketch，它的 bottom-8 应该是这些值。

------

### **这有啥用？**

- 你不需要真的合并 A 和 B 的完整集合；
- 只用两个 sketch 就能合并出一个 **近似的并集 sketch**；
- 这个技巧使得我们可以高效地估计：
  - 并集的大小；
  - 交集的大小；
  - 甚至支持分布式计算（每台机器发来它的 sketch）。

------

### **补充说明**

- 下方时间线帮助你看清楚重叠的元素（比如 3、7、11、17、23）；
- 这也说明为何 sketch 越长（即 k 越大），估计越准确。

------

需要我总结一下 MinHash 的整体用途和核心思想吗？